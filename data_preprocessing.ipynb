{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = r'C:\\Users\\reemk\\aiincbs\\MachineLearningCVE'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading: Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\n",
      "Loading: Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\n",
      "Loading: Friday-WorkingHours-Morning.pcap_ISCX.csv\n",
      "Loading: Monday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\n",
      "Loading: Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\n",
      "Loading: Tuesday-WorkingHours.pcap_ISCX.csv\n",
      "Loading: Wednesday-workingHours.pcap_ISCX.csv\n",
      "\n",
      " Combined shape: (2830743, 79)\n"
     ]
    }
   ],
   "source": [
    "#appending all csv files into 1 dataframe\n",
    "csv_files = [f for f in os.listdir(dataset_path) if f.endswith('.csv')]\n",
    "\n",
    "df_list = []\n",
    "for filename in csv_files:\n",
    "    file_path = os.path.join(dataset_path, filename)\n",
    "    print(f\"Loading: {filename}\")\n",
    "    df = pd.read_csv(file_path, low_memory=False)\n",
    "    df_list.append(df)\n",
    "\n",
    "df_full = pd.concat(df_list, ignore_index=True)\n",
    "print(f\"\\n Combined shape: {df_full.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Data Exploration</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([' Destination Port', ' Flow Duration', ' Total Fwd Packets',\n",
       "       ' Total Backward Packets', 'Total Length of Fwd Packets',\n",
       "       ' Total Length of Bwd Packets', ' Fwd Packet Length Max',\n",
       "       ' Fwd Packet Length Min', ' Fwd Packet Length Mean',\n",
       "       ' Fwd Packet Length Std', 'Bwd Packet Length Max',\n",
       "       ' Bwd Packet Length Min', ' Bwd Packet Length Mean',\n",
       "       ' Bwd Packet Length Std', 'Flow Bytes/s', ' Flow Packets/s',\n",
       "       ' Flow IAT Mean', ' Flow IAT Std', ' Flow IAT Max', ' Flow IAT Min',\n",
       "       'Fwd IAT Total', ' Fwd IAT Mean', ' Fwd IAT Std', ' Fwd IAT Max',\n",
       "       ' Fwd IAT Min', 'Bwd IAT Total', ' Bwd IAT Mean', ' Bwd IAT Std',\n",
       "       ' Bwd IAT Max', ' Bwd IAT Min', 'Fwd PSH Flags', ' Bwd PSH Flags',\n",
       "       ' Fwd URG Flags', ' Bwd URG Flags', ' Fwd Header Length',\n",
       "       ' Bwd Header Length', 'Fwd Packets/s', ' Bwd Packets/s',\n",
       "       ' Min Packet Length', ' Max Packet Length', ' Packet Length Mean',\n",
       "       ' Packet Length Std', ' Packet Length Variance', 'FIN Flag Count',\n",
       "       ' SYN Flag Count', ' RST Flag Count', ' PSH Flag Count',\n",
       "       ' ACK Flag Count', ' URG Flag Count', ' CWE Flag Count',\n",
       "       ' ECE Flag Count', ' Down/Up Ratio', ' Average Packet Size',\n",
       "       ' Avg Fwd Segment Size', ' Avg Bwd Segment Size',\n",
       "       ' Fwd Header Length.1', 'Fwd Avg Bytes/Bulk', ' Fwd Avg Packets/Bulk',\n",
       "       ' Fwd Avg Bulk Rate', ' Bwd Avg Bytes/Bulk', ' Bwd Avg Packets/Bulk',\n",
       "       'Bwd Avg Bulk Rate', 'Subflow Fwd Packets', ' Subflow Fwd Bytes',\n",
       "       ' Subflow Bwd Packets', ' Subflow Bwd Bytes', 'Init_Win_bytes_forward',\n",
       "       ' Init_Win_bytes_backward', ' act_data_pkt_fwd',\n",
       "       ' min_seg_size_forward', 'Active Mean', ' Active Std', ' Active Max',\n",
       "       ' Active Min', 'Idle Mean', ' Idle Std', ' Idle Max', ' Idle Min',\n",
       "       ' Label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Removing Wrapping Spaces, extra spaces, etc</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.columns = (\n",
    "    df_full.columns\n",
    "    .str.strip()                 \n",
    "    .str.lower()                 \n",
    "    .str.replace(' ', '_')      \n",
    "    .str.replace('-', '_')       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['destination_port', 'flow_duration', 'total_fwd_packets',\n",
       "       'total_backward_packets', 'total_length_of_fwd_packets',\n",
       "       'total_length_of_bwd_packets', 'fwd_packet_length_max',\n",
       "       'fwd_packet_length_min', 'fwd_packet_length_mean',\n",
       "       'fwd_packet_length_std', 'bwd_packet_length_max',\n",
       "       'bwd_packet_length_min', 'bwd_packet_length_mean',\n",
       "       'bwd_packet_length_std', 'flow_bytes/s', 'flow_packets/s',\n",
       "       'flow_iat_mean', 'flow_iat_std', 'flow_iat_max', 'flow_iat_min',\n",
       "       'fwd_iat_total', 'fwd_iat_mean', 'fwd_iat_std', 'fwd_iat_max',\n",
       "       'fwd_iat_min', 'bwd_iat_total', 'bwd_iat_mean', 'bwd_iat_std',\n",
       "       'bwd_iat_max', 'bwd_iat_min', 'fwd_psh_flags', 'bwd_psh_flags',\n",
       "       'fwd_urg_flags', 'bwd_urg_flags', 'fwd_header_length',\n",
       "       'bwd_header_length', 'fwd_packets/s', 'bwd_packets/s',\n",
       "       'min_packet_length', 'max_packet_length', 'packet_length_mean',\n",
       "       'packet_length_std', 'packet_length_variance', 'fin_flag_count',\n",
       "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
       "       'urg_flag_count', 'cwe_flag_count', 'ece_flag_count', 'down/up_ratio',\n",
       "       'average_packet_size', 'avg_fwd_segment_size', 'avg_bwd_segment_size',\n",
       "       'fwd_header_length.1', 'fwd_avg_bytes/bulk', 'fwd_avg_packets/bulk',\n",
       "       'fwd_avg_bulk_rate', 'bwd_avg_bytes/bulk', 'bwd_avg_packets/bulk',\n",
       "       'bwd_avg_bulk_rate', 'subflow_fwd_packets', 'subflow_fwd_bytes',\n",
       "       'subflow_bwd_packets', 'subflow_bwd_bytes', 'init_win_bytes_forward',\n",
       "       'init_win_bytes_backward', 'act_data_pkt_fwd', 'min_seg_size_forward',\n",
       "       'active_mean', 'active_std', 'active_max', 'active_min', 'idle_mean',\n",
       "       'idle_std', 'idle_max', 'idle_min', 'label'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Checking for missing or infinite values</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flow_bytes/s    1358\n",
      "dtype: int64\n",
      "flow_bytes/s      1509\n",
      "flow_packets/s    2867\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values = df_full.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "infinite_values = df_full.isin([np.inf, -np.inf]).sum()\n",
    "print(infinite_values[infinite_values > 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>it seems that classes flow_bytes/s and flow_packets/s got 0s and inf values, dropping them</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full= df_full.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df_full= df_full.dropna(subset=['flow_bytes/s', 'flow_packets/s'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Dropping all duplicates in records</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 307078\n",
      "Duplicate rows: 307078 out of 2827876 (10.86%)\n"
     ]
    }
   ],
   "source": [
    "#view duplicate rows (across all columns)\n",
    "num_duplicates = df_full.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {num_duplicates}\")\n",
    "\n",
    "duplicate_percentage = (num_duplicates / len(df_full)) * 100\n",
    "print(f\"Duplicate rows: {num_duplicates} out of {len(df_full)} ({duplicate_percentage:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>since its 10%, relatively low %, so ill choose to drop duplicate records, they wont add value to model training anyway</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full= df_full.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
